{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab Cheatsheet\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marcinsawinski/UEP_KIE_ML_LAB_PROG/blob/main/00_cheatsheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from url\n",
    "import urllib.request\n",
    "\n",
    "#shift image\n",
    "from scipy.ndimage import shift\n",
    "\n",
    "\n",
    "# viz an data libs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# preprocessing and impute \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "# create and compose pipelines\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "# models for regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# models for classification\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# split and evluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "# metrics for regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# metrics for classification \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# custom code\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.utils.validation import check_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# sklearn datasets\n",
    "from sklearn.datasets import fetch_openml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.1\n",
    "url_lifesat = 'https://github.com/marcinsawinski/UEP_KIE_ML_LAB_PROG/raw/main/datasets/lifesat/lifesat.csv'\n",
    "# Task 1.3\n",
    "url_gdb = 'https://github.com/marcinsawinski/UEP_KIE_ML_LAB_PROG/raw/main/datasets/lifesat/gdp-per-capita-worldbank.csv'\n",
    "url_oecd = 'https://github.com/marcinsawinski/UEP_KIE_ML_LAB_PROG/raw/main/datasets/lifesat/oecd_bli_20221109.csv'\n",
    "\n",
    "# Task 2\n",
    "url_housing = 'https://github.com/marcinsawinski/UEP_KIE_ML_LAB_PROG/raw/main/datasets/housing/housing.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch file from url \n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# map list values from labels to bool\n",
    "y_nn = (y == 'nn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create, copy and merge dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read remote or local csv into a pandas dataframe\n",
    "df = pd.read_csv('file or url address')\n",
    "\n",
    "# Deepcopy dataframe df_A to dataframe df_B (full copy, not reference)\n",
    "df_B = df_A.copy()\n",
    "\n",
    "# Shallow copy dataframe df_A to dataframe df_B (just reference)\n",
    "df_B = df_A\n",
    "\n",
    "# Shallow copy subset of dataframe df_A to dataframe df_B (column1 and column2)\n",
    "df_B = df_A[['column1','column2']]\n",
    "\n",
    "### Merge dataframes df_a and df_b into df_c (like sql join)\n",
    "# using matching indexes\n",
    "df_c = pd.merge(left=df_a, right=df_b, left_index=True, right_index=True)\n",
    "# using key column column1\n",
    "df_c = pd.merge(left=df_a, right=df_b, on='column1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature column3 calculated as column1/column2\n",
    "df[\"column3\"] = df[\"column1\"] / df[\"column2\"]\n",
    "\n",
    "# Drop columns in single dataframe\n",
    "df.drop(columns=['column1','column2'] inplace=True)\n",
    "# fDrop columns or many dataframes\n",
    "for set_ in (df1, df2):\n",
    "    set_.drop(columns=['column1','column2'] inplace=True)\n",
    "\n",
    "# Generate categories (manually specified bins 1-5 as ranges 0-1.5, 1.5-3, 3-4.5, etc)\n",
    "df[\"category_col\"] = pd.cut(df[\"numeric_col\"],\n",
    "                               bins=[0, 1.5, 3, 4.5, 6, np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Generate categories (automatically 1 bin for each percentile)\n",
    "percentiles = [np.percentile(df[\"column1\"], p) for p in range(1, 100)]\n",
    "flattened_column1 = pd.cut(df[\"column1\"],\n",
    "                    bins=[-np.inf] + percentiles + [np.inf],\n",
    "                    labels=range(1, 100 + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and convert data from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pick specific columns (column1 and column2)\n",
    "cols = ['column1','column2']\n",
    "df[cols]\n",
    "# or one line\n",
    "df[['column1','column2']]\n",
    "\n",
    "### Pick rows that fill specifi criteria e.g. rows where column1 is equal 100\n",
    "df[df.column1 == 100]\n",
    "#or\n",
    "df[df['column1'] == 100]\n",
    "\n",
    "### Convert pandas to numpy array\n",
    "df.values \n",
    "# or\n",
    "df.to_numpy()\n",
    "\n",
    "### Pick numnerical columns\n",
    "df_num = df.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview and visualize data in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first lines\n",
    "df.head()\n",
    "\n",
    "# Columns type and count summary\n",
    "df.info() \n",
    "\n",
    "# Categorical column - values and counts\n",
    "df[\"category_col\"].value_counts() \n",
    "\n",
    "# Categorical column - bins size share\n",
    "df[\"category_col\"].value_counts() / len(df)\n",
    "\n",
    "# Numberical columns - basic statistics\n",
    "df.describe() \n",
    "\n",
    "# Check for null values\n",
    "df.isna().sum()\n",
    "\n",
    "# Find null values\n",
    "null_rows_idx = df.isnull().any(axis=1)\n",
    "df.loc[null_rows_idx]\n",
    "# or\n",
    "df[df.isna().any(axis=1)]\n",
    "\n",
    "# Histograms for all columns in dataframe\n",
    "df.hist(bins=50, figsize=(12, 8)) \n",
    "\n",
    "# Visualize dataframe as basic scatter plot\n",
    "df.plot(kind='scatter', grid=True,\n",
    "             x=\"column1\", y=\"column2\")\n",
    "\n",
    "# Visualize dataframe as scatter plot with extra options\n",
    "df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=1, \n",
    "s=df[\"column1\"] ,c=\"column2\",cmap=\"jet\", figsize=(10, 7),\n",
    "legend=True, colorbar=True,label=\"column2\")\n",
    "\n",
    "# Correlation table\n",
    "df[['column1','column2']].corr()\n",
    "\n",
    "# Correlation list\n",
    "corr_matrix = df[['column1','column2']].corr()\n",
    "corr_matrix[\"column1\"].sort_values(ascending=False)\n",
    "\n",
    "# Correlation plot\n",
    "df.plot(kind=\"scatter\", x=\"column1\", y=\"column2\",\n",
    "             alpha=0.1, grid=True)\n",
    "\n",
    "# Scatter matrix\n",
    "scatter_matrix(df[['column1','column2']], figsize=(12, 8))\n",
    "\n",
    "# Median values\n",
    "df.median().values\n",
    "\n",
    "# Visualize category column \n",
    "df[\"category_col\"].value_counts().sort_index().plot.bar(rot=0)\n",
    "\n",
    "# Show plot (optional in Jupyter)\n",
    "plt.show()\n",
    "\n",
    "# Show 2 sublots side by side, original value and transfomred with log function\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 3), sharey=True)\n",
    "df[\"column1\"].hist(ax=axs[0], bins=50)\n",
    "df[\"column1\"].apply(np.log).hist(ax=axs[1], bins=50)\n",
    "axs[0].set_xlabel(\"Feature 1\")\n",
    "axs[1].set_xlabel(\"Feature 1 log\")\n",
    "axs[0].set_ylabel(\"Label\")\n",
    "plt.show()\n",
    "\n",
    "# Show RBF gamma parameter\n",
    "col1_range = np.linspace(df[\"column1\"].min(),\n",
    "                   df[\"column1\"].max(),\n",
    "                   500).reshape(-1, 1)\n",
    "gamma1 = 0.1\n",
    "gamma2 = 0.03\n",
    "rbf1 = rbf_kernel(col1_range, [[35]], gamma=gamma1)\n",
    "rbf2 = rbf_kernel(col1_range, [[35]], gamma=gamma2)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel(\"Feature 1\")\n",
    "ax1.set_ylabel(\"Label\")\n",
    "ax1.hist(housing[\"column1\"], bins=50)\n",
    "\n",
    "ax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\n",
    "color = \"blue\"\n",
    "ax2.plot(col1_range, rbf1, color=color, label=\"gamma = 0.10\")\n",
    "ax2.plot(col1_range, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylabel(\"Feature 1 similarity\", color=color)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset e.g. mnist_784\n",
    "data = fetch_openml('mnist_784', as_frame=False)\n",
    "# check content\n",
    "data.keys() \n",
    "# assign X and y\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create ML model\n",
    "# Create Linear Regression Model\n",
    "model = LinearRegression()\n",
    "# Create Regression Model based on k-nearest neighbors with k=5\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Create SGD classifier\n",
    "model = SGDClassifier()\n",
    "# Create RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "# Create KNeighborsClassifier\n",
    "model = knn_clf = KNeighborsClassifier()\n",
    "\n",
    "# Fit model with independant variables X and dependant variable y\n",
    "model.fit(X, y)\n",
    "\n",
    "# Score model\n",
    "score = model.score(X, y)\n",
    "\n",
    "# Predict using  model for one new values (one element matrix with value 100\n",
    "x_n = [[100]]\n",
    "model.predict(x_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train /test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random train /test split 80/20\n",
    "train_set, test_set = train_test_split(df, test_size=0.2)\n",
    "\n",
    "### Stratified train /test split 80/20 using category_column\n",
    "# single split\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=df[\"category_col\"])\n",
    "\n",
    "# or mulitple splits\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "strat_splits = []\n",
    "for train_index, test_index in splitter.split(df, df[\"category_col\"]):\n",
    "    strat_train_set_n = df.iloc[train_index]\n",
    "    strat_test_set_n = df.iloc[test_index]\n",
    "    strat_splits.append([strat_train_set_n, strat_test_set_n])\n",
    "#pick sets from first split \n",
    "strat_train_set, strat_test_set = strat_splits[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of **numerical** features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input missing data (median) on 2 columns\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(df_num)\n",
    "X = imputer.transform(df['num_col1', 'num_col2'])\n",
    "# Check values (not required)\n",
    "imputer.statistics_\n",
    "# Back from array to DF (not required)\n",
    "df_tr = pd.DataFrame(X, columns=df.columns,\n",
    "                          index=df.index)\n",
    "\n",
    "# Normalize numnerical values\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_min_max_scaled = min_max_scaler.fit_transform(df_num)\n",
    "\n",
    "# Standardize numnerical values\n",
    "std_scaler = StandardScaler()\n",
    "X_std_scaled = std_scaler.fit_transform(df_num)\n",
    "# Back from array to DF (not required)\n",
    "df = pd.DataFrame(X_std_scaled, columns=df_num.columns, index=df_num.index)\n",
    "\n",
    "# Feature logaritmic transformation \n",
    "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\n",
    "log_data = log_transformer.transform(df[[\"column1\", \"column2\"]])\n",
    "\n",
    "# Create new feature as distnce for a fixed point(base = 35) in column 1 using RBF with gamma 0.1\n",
    "col1_simil_35 = rbf_kernel(df[[\"column1\"]], [[35]], gamma=0.1)\n",
    "# or \n",
    "rbf_transformer = FunctionTransformer(rbf_kernel,\n",
    "                                      kw_args=dict(Y=[[35.]], gamma=0.1))\n",
    "column1_simil_35 = rbf_transformer.transform(df[[\"column1\"]])\n",
    "\n",
    "# Create new feature as distnce for a fixed point(base = 37.7749, -122.41) in 2 columns using RBF with gamma 0.1\n",
    "base = 37.7749, -122.41\n",
    "base_transformer = FunctionTransformer(rbf_kernel,\n",
    "                                     kw_args=dict(Y=[base], gamma=0.1))\n",
    "base_simil = sf_transformer.transform(df[[\"column1\", \"column2\"]])\n",
    "\n",
    "# Ratio transformer\n",
    "ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\n",
    "ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of **categorical** features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode ordinal categories\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X_cat_encoded = ordinal_encoder.fit_transform(df_cat)\n",
    "# Check values (not required)\n",
    "ordinal_encoder.categories_\n",
    "# Back to DF and count (not required)\n",
    "\n",
    "pd.DataFrame(X_cat_encoded, columns=df_cat.columns,\n",
    "                index=df_cat.index).value_counts()\n",
    "\n",
    "# Encode non-ordinal categories\n",
    "cat_encoder = OneHotEncoder()\n",
    "X_cat_1hot = cat_encoder.fit_transform(df_cat)\n",
    "# note:  By default, the `OneHotEncoder` class returns a sparse array, \n",
    "# but we can convert it to a dense array if needed by calling the `toarray()` method:\n",
    "X_cat_1hot.toarray()\n",
    "# or alternatively, you can set `sparse=False` when creating the `OneHotEncoder`:\n",
    "cat_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Ignore new categories\n",
    "cat_encoder.handle_unknown = \"ignore\"\n",
    "# or \n",
    "cat_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Check encoder (not required)\n",
    "cat_encoder.categories_\n",
    "cat_encoder.feature_names_in_\n",
    "cat_encoder.get_feature_names_out()\n",
    "# Back to DF \n",
    "df_output = pd.DataFrame(cat_encoder.transform(df).toarray(),\n",
    "                         columns=cat_encoder.get_feature_names_out(),\n",
    "                         index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of **outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "isolation_forest = IsolationForest(random_state=42)\n",
    "outlier_pred = isolation_forest.fit_predict(X)\n",
    "df_tr[outlier_pred==1]\n",
    "# outliers\n",
    "df_tr[outlier_pred==-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize target (labels)\n",
    "target_scaler = StandardScaler()\n",
    "scaled_labels = target_scaler.fit_transform(labels.to_frame())\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(df_num, scaled_labels)\n",
    "new_data = df_num.iloc[:5]  # pretend this is new data\n",
    "\n",
    "# manual inversion\n",
    "scaled_predictions = model.predict(new_data)\n",
    "predictions = target_scaler.inverse_transform(scaled_predictions)\n",
    "\n",
    "# automatic inversion\n",
    "model = TransformedTargetRegressor(LinearRegression(),\n",
    "                                   transformer=StandardScaler())\n",
    "model.fit(df_num, labels)\n",
    "predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Add custom columns_\n",
    "- col1_2 = column1 / column2 (always)\n",
    "- col3_4 = column3 / column4 (optional when hyperparamter add_col3_4 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index for columns\n",
    "col_names = \"colum1\", \"colum2\", \"column3\", \"column4\"\n",
    "col1_ix, col2_ix, col3_ix, col4_ix = [\n",
    "    df.columns.get_loc(c) for c in col_names]\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_col3_4=True): # no *args or **kargs\n",
    "        self.add_col3_4 = add_col3_4\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        col1_2 = X[:, col1_ix] / X[:, col2_ix]\n",
    "        if self.add_col3_4:\n",
    "            col3_4 = X[:, col3_ix] / X[:, col4_ix]\n",
    "            return np.c_[X, col1_2, col3_4]\n",
    "        else:\n",
    "            return np.c_[X, col1_2]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_col3_4=False)\n",
    "X_extra_attribs = attr_adder.transform(df.values)\n",
    "\n",
    "# Back to dataframe\n",
    "df_extra_attribs = pd.DataFrame(\n",
    "    X_extra_attribs,\n",
    "    columns=list(df.columns)+[\"col1_2\", \"add_col3_4\"],\n",
    "    index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Custom transformer - clone of StandardScaler_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
    "        X = check_array(X)  # checks that X is an array with finite float values\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.scale_ = X.std(axis=0)\n",
    "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
    "        X = check_array(X)\n",
    "        assert self.n_features_in_ == X.shape[1]\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Custom transformation for cluster similarity (define n clusters and add n features as similarity to each cluser using RBF with gamma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Set sklearn utils_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set configuration paramters\n",
    "set_config(display='diagram')\n",
    "\n",
    "# check steps for pipline called num_pipeline \n",
    "num_pipeline.steps\n",
    "\n",
    "# display 2nd step for pipline called num_pipeline \n",
    "num_pipeline[1]\n",
    "\n",
    "# display simpleimputer step for pipline called num_pipeline \n",
    "num_pipeline.named_steps[\"simpleimputer\"]\n",
    "\n",
    "# set simpleimputer strategy paramter for pipline called num_pipeline \n",
    "num_pipeline.set_params(simpleimputer__strategy=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Numeric feature pipeline with median inputer  and stanardization scaler_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_tr = num_pipeline.fit_transform(df_num)\n",
    "# or\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Numeric feature pipeline with median inputer, attributes adder and stanardization scaler_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_tr = num_pipeline.fit_transform(df_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Compose pipeline (num_pipline for numeric features and OneHotEncoder for cat features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"column1\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "X_prepared = full_pipeline.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Compose 2 pipelines (num_pipline and cat_pipeline)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"), \n",
    "    StandardScaler())\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "num_attribs = [\"column1\",\"column2\"]\n",
    "cat_attribs = [\"column3\",\"column4\"]\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "#or\n",
    "preprocessing = make_column_transformer(\n",
    "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
    ")\n",
    "\n",
    "#call pipline\n",
    "X_prepared = preprocessing.fit_transform(df)\n",
    "\n",
    "# Back to dataframe (not required)\n",
    "df_prepared = pd.DataFrame(\n",
    "    X_prepared,\n",
    "    columns=preprocessing.get_feature_names_out(),\n",
    "    index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select an train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Process data, train model calcualte RSME_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = strat_train_set.drop(columns=\"label\")\n",
    "y_labels = strat_train_set[\"label\"].copy()\n",
    "\n",
    "num_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"), \n",
    "    StandardScaler())\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "preprocessing = make_column_transformer(\n",
    "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
    ")\n",
    "X_prepared = preprocessing.fit_transform(df)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_prepared, y_labels)\n",
    "y_predictions = lin_reg.predict(X_prepared)\n",
    "\n",
    "# Calculate RSME\n",
    "lin_mse = mean_squared_error(y_labels, y_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "# or \n",
    "mean_squared_error(y_labels, y_predictions, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Perforem train and validation for Decision Tree_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_prepared, y_labels)\n",
    "\n",
    "y_predictions = tree_reg.predict(X_prepared)\n",
    "tree_rmse = mean_squared_error(y_labels, y_predictions, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Perforem train and validation for Random forest_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg = RandomForestRegressor(n_estimators=100)\n",
    "forest_reg.fit(X_prepared, y_labels)\n",
    "\n",
    "y_predictions = forest_reg.predict(X_prepared)\n",
    "forest_mse = mean_squared_error(y_labels, y_predictions, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Cross validation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation for regression\n",
    "scores = cross_val_score(tree_reg, X_prepared, y_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)\n",
    "\n",
    "\n",
    "#cross validation for classification\n",
    "cross_val_score(model, X, y, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model (e.g. KNeighborsClassifier)\n",
    "model = knn_clf = KNeighborsClassifier()\n",
    "# set grid of hyperparamters values\n",
    "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5, 6]}]\n",
    "\n",
    "# fit all models ( one per param_grid combination) \n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_small, y_small)\n",
    "\n",
    "# check which hyperparamters values are best\n",
    "grid_search.best_params_\n",
    "\n",
    "# check score with best hyperparamters values\n",
    "grid_search.best_score_\n",
    "\n",
    "# reuse model with best hyperparamters values\n",
    "grid_search.best_estimator_.fit(X_all, y_all)\n",
    "\n",
    "# check score on test data\n",
    "score = grid_search.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate confusion matrix\n",
    "confusion_matrix(y, y_pred)\n",
    "\n",
    "# calculateprecision\n",
    "precision_score(y, y_pred)\n",
    "\n",
    "# calculate recall\n",
    "recall_score(y, y_pred)\n",
    "\n",
    "# calculate f1\n",
    "f1_score(y, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tresholds and curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores from decision_function (e.g. SVM)\n",
    "y_scores = cross_val_predict(model, X, y, cv=3,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "# get class probabilities from decision_function (e.g. RandomForestClassifier)\n",
    "y_probas = cross_val_predict(model, X, y, cv=3,\n",
    "                             method=\"predict_proba\")\n",
    "# get scores from probas\n",
    "y_scores = y_probas[:, 1]\n",
    "\n",
    "# calculate precisions, recalls for thresholds for precision-recall plot\n",
    "precisions, recalls, thresholds = precision_recall_curve(y, y_scores)\n",
    "\n",
    "# calculate  fpr, tpr, thresholds for ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a treshold to visualize\n",
    "threshold = 1000\n",
    "# get index of first pos ≥ threshold\n",
    "idx = (thresholds >= threshold).argmax()\n",
    "# plot precision, recall vs threshold \n",
    "plt.figure(figsize=(8, 4)) \n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "\n",
    "plt.plot(thresholds[idx], precisions[idx], \"bo\")\n",
    "plt.plot(thresholds[idx], recalls[idx], \"go\")\n",
    "plt.axis([-50000, 50000, 0, 1])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"center right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make precision-recall plot\n",
    "plt.figure(figsize=(6, 5))  \n",
    "\n",
    "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n",
    "\n",
    "plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n",
    "plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\n",
    "plt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n",
    "         label=\"Point at threshold 3,000\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid()\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\n",
    "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n",
    "\n",
    "plt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n",
    "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
    "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
    "\n",
    "plt.xlabel('False Positive Rate (Fall-Out)')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.grid()\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(loc=\"lower right\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc AUC\n",
    "roc_auc_score(y, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what would be treshold for 90% precision?\n",
    "idx_for_90_precision = (precisions >= 0.90).argmax()\n",
    "threshold_for_90_precision = thresholds[idx_for_90_precision]\n",
    "threshold_for_90_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform scores to labels using  90% precision treshold\n",
    "y_train_pred_90 = (y_scores >= threshold_for_90_precision)\n",
    "# check precision\n",
    "precision_score(y_train_5, y_train_pred_90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift image\n",
    "data = np.array([0,0,0,1,1,1,0,0,0])\n",
    "image = data.reshape((3, 3))\n",
    "shifted_image = shift(image, [1, 1], cval=0, mode=\"constant\")\n",
    "shifted_image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hml3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac2598a53cd48ed973662853cbed9ce85601c819c2e7e5e54efa32ca245c1cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
