{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general\n",
    "import urllib.request\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# preprocess\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#evluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_lifesat = 'https://github.com/marcinsawinski/UEP_KIE_ML_LAB_PROG/raw/main/datasets/lifesat/lifesat.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch file from url \n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read remote or local csv into a pandas dataframe\n",
    "df = pd.read_csv('file or url address')\n",
    "\n",
    "# Pick specific columns\n",
    "df[['column1','column2']]\n",
    "\n",
    "# Drop column for many dataframes\n",
    "for set_ in (df1, df2):\n",
    "    set_.drop(columns=\"column\" inplace=True)\n",
    "\n",
    "#Pick rows that fill specifi criteria e.g. rows where column1 is equal 100\n",
    "df[df.column1 == 100]\n",
    "\n",
    "# Convert pandas to numpy array\n",
    "df.values \n",
    "#or\n",
    "df.to_numpy()\n",
    "\n",
    "# Visualize dataframe as scatter plot\n",
    "df.plot(kind='scatter', grid=True,\n",
    "             x=\"column1\", y=\"column2\")\n",
    "\n",
    "# Visualize dataframe as scatter plot extra\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=1, \n",
    "s=housing[\"col1\"] ,c=\"col3\",cmap=\"jet\", figsize=(10, 7),\n",
    "legend=True, colorbar=True,label=\"col3\")\n",
    "\n",
    "\n",
    "# Merge dataframes df_a and df_b into df_c using matching indexes\n",
    "df_c = pd.merge(left=df_a, right=df_b, left_index=True, right_index=True)\n",
    "\n",
    "# Merge dataframes df_a and df_b into df_c using key column column1\n",
    "df_c = pd.merge(left=df_a, right=df_b, on='column1')\n",
    "\n",
    "#Preview dataset\n",
    "df.head()\n",
    "df.info()\n",
    "df[\"column1\"].value_counts()\n",
    "df.describe()\n",
    "df.hist(bins=50, figsize=(12, 8))\n",
    "df.isna().sum()\n",
    "\n",
    "# Correlation table\n",
    "df[['col1','clo2']].corr()\n",
    "\n",
    "# Correlation list\n",
    "corr_matrix = df[['col1','clo2']].corr()\n",
    "corr_matrix[\"col1\"].sort_values(ascending=False)\n",
    "\n",
    "# Correlation plot\n",
    "df.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1, grid=True)\n",
    "\n",
    "# Scatter matrix\n",
    "scatter_matrix(df[['col1','clo2']], figsize=(12, 8))\n",
    "\n",
    "# median values\n",
    "df.median().values\n",
    "\n",
    "# Generate categories\n",
    "df[\"category_col\"] = pd.cut(df[\"numeric_col\"],\n",
    "                               bins=[0, 1.5, 3, 4.5, 6, np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "# Value counts\n",
    "df[\"category_col\"].value_counts()\n",
    "\n",
    "# Bins size\n",
    "df[\"category_col\"].value_counts() / len(df)\n",
    "\n",
    "# Visualize\n",
    "df[\"category_col\"].value_counts().sort_index().plot.bar(rot=0)\n",
    "\n",
    "# Find null values\n",
    "null_rows_idx = df.isnull().any(axis=1)\n",
    "df.loc[null_rows_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Linear Regression Model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create Regression Model based on k-nearest neighbors with k=5\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Fit model with independant variables a and dependant variable b\n",
    "model.fit(a, b)\n",
    "\n",
    "# Predict using  model for one new values (one element matrix with value 100\n",
    "x_n = [[100]]\n",
    "model.predict(x_n)\n",
    "\n",
    "# Random train /test split\n",
    "train_set, test_set = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Stratified train /test split\n",
    "strat_train_set, strat_test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=df[\"category_col\"])\n",
    "# or\n",
    "for train_index, test_index in splitter.split(df, df[\"category_col\"]):\n",
    "    strat_train_set_n = df.iloc[train_index]\n",
    "    strat_test_set_n = df.iloc[test_index]\n",
    "    strat_splits.append([strat_train_set_n, strat_test_set_n])\n",
    "strat_train_set, strat_test_set = strat_splits[0]\n",
    "\n",
    "\n",
    "# Input missing data (median)\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(df_num)\n",
    "# Check values\n",
    "imputer.statistics_\n",
    "X = imputer.transform(df['num_col1', 'num_col2'])\n",
    "# Back to DF\n",
    "df_tr = pd.DataFrame(X, columns=df.columns,\n",
    "                          index=df.index)\n",
    "\n",
    "# Encode ordinal categories\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "df_cat_encoded = ordinal_encoder.fit_transform(df_cat)\n",
    "ordinal_encoder.categories_\n",
    "# Back to DF and count\n",
    "pd.DataFrame(df_cat_encoded, columns=df_cat.columns,\n",
    "                index=df_cat.index).value_counts()\n",
    "\n",
    "# Encode non-ordinal categories\n",
    "cat_encoder = OneHotEncoder()\n",
    "df_cat_1hot = cat_encoder.fit_transform(df_cat)\n",
    "df_cat_1hot.toarray()\n",
    "\n",
    "# Ignore new categories\n",
    "cat_encoder.handle_unknown = \"ignore\"\n",
    "\n",
    "# Check encoder\n",
    "cat_encoder.categories_\n",
    "cat_encoder.feature_names_in_\n",
    "cat_encoder.get_feature_names_out()\n",
    "\n",
    "# Remove outliers\n",
    "isolation_forest = IsolationForest(random_state=42)\n",
    "outlier_pred = isolation_forest.fit_predict(X)\n",
    "df_tr[outlier_pred==1]\n",
    "# outliers\n",
    "df_tr[outlier_pred==-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('hml3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac2598a53cd48ed973662853cbed9ce85601c819c2e7e5e54efa32ca245c1cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
